{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1757d2b0-cc0c-44a5-9a79-0c6defb72477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import numpy as np\n",
    "from langchain_together import Together\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60aa7efe-b740-4d08-9ae7-2d5a6118aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# for multiple pdf files\n",
    "loaders = [\n",
    "    # Duplicate documents on purpose - messy data\n",
    "    PyPDFLoader(\"file_1.pdf\"),\n",
    "    PyPDFLoader(\"file_1.pdf\"),\n",
    "    PyPDFLoader(\"file_2.pdf\"),\n",
    "    PyPDFLoader(\"file_3.pdf\")\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2375b6b-3c4e-4a1a-bf67-6a260985c6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Supervised Learning  \\nSupervised learning involves training a model on a labeled dataset, which means that each \\ntraining example is paired with an output label. The goal is to learn a mapping from inputs to \\noutputs that can be used to predict the labels of new, unseen examples.  \\nClassification  \\nClassification is a type of supervised learning where the goal is to assign inputs into one of \\nseveral predefined categories.  \\n• Binary Classification : The model distinguishes between two classes. Examples include \\nspam detection (spam or not spam) and disease diagnosis (diseased or healthy).  \\n• Multiclass Classification : The model distinguishes among three or more classes. \\nExamples include handwriting recognition (recognizing each digit from 0 to 9) and image \\nclassification (identifying objects in images).  \\nCommon algorithms for classification include:  \\n• Logistic Regression : Models the probability of a binary outcome using a logistic \\nfunction.  \\n• Decision Trees : Splits the data into subsets based on the value of input features.  \\n• Support Vector Machines (SVM) : Finds the hyperplane that best separates the classes \\nin the feature space.  \\n• k-Nearest Neighbors (k -NN): Classifies a data point based on the majority class among \\nits k-nearest neighbors.  \\nRegression  \\nRegression is a type of supervised learning where the goal is to predict a continuous output.  \\n• Linear Regression : Models the relationship between the input variables and the output \\nby fitting a linear equation to the observed data.  \\n• Polynomial Regression : Extends linear regression by considering polynomial \\nrelationships between the input variables and the output.  \\n• Ridge and Lasso Regression : Variants of linear regression that include regularization \\nterms to prevent overfitting.  \\nCommon metrics for regression include Mean Squared Error (MSE), Root Mean Squared Error \\n(RMSE), and Mean Absolute Error (MAE).  \\nEnsemble Methods  \\nEnsemble methods combine multiple learning algorithms to obtain better predictive performance \\nthan could be obtained from any of the constituent models alone.  ', metadata={'source': 'file_1.pdf', 'page': 0}),\n",
       " Document(page_content='• Random Forests : An ensemble of decision trees, typically trained with the \"bagging\" \\nmethod. Each tree in the forest is trained on a subset of the data, and the final prediction \\nis made by averaging the predictions of all the trees.  \\n• Gradient Boosting : Builds an ensemble of trees sequentially, where each new tree \\nfocuses on correcting the errors made by the previous trees. Examples include Gradient \\nBoosting Machines (GBM) and XGBoost.  \\nSupport Vector Machines (SVM)  \\nSVMs are supervised learning models that analyze data for classification and regression analysis. \\nThe core idea is to find a hyperplane in a high -dimensional space that distinctly classifies the \\ndata points.  \\n• Linear SVM : Uses a linear hyperplane to separate the data.  \\n• Non-linear SVM : Uses kernel functions (e.g., RBF, polynomial) to project data into \\nhigher dimensions where a linear separation is possible.  \\nk-Nearest Neighbors (k -NN) \\nk-NN is a simple, instance -based learning algorithm where the class of a data point is determined \\nby the majority class of its k -nearest neighbors in the feature space. It is computationally \\nexpensive during prediction since it involves calculating the dis tance to all training points.  \\nNeural Networks  \\nNeural networks are a set of algorithms inspired by the structure and function of the human \\nbrain, designed to recognize patterns.  \\n• Feedforward Neural Networks (FNN) : The simplest type of neural network where \\ninformation moves in one direction —from input to output.  \\n• Convolutional Neural Networks (CNNs) : Primarily used for image data, CNNs use \\nconvolutional layers to automatically and adaptively learn spatial hierarchies of features.  \\n• Recurrent Neural Networks (RNNs) : Suitable for sequential data, RNNs have \\nconnections that form directed cycles, allowing them to maintain a memory of previous \\ninputs. Long Short -Term Memory (LSTM) networks are a type of RNN designed to \\nhandle long -term dependencies.  \\nUnsupervised Learning  \\nUnsupervised learning involves training a model on data without labeled responses. The goal is \\nto infer the natural structure present within a set of data points.  \\nClustering  ', metadata={'source': 'file_1.pdf', 'page': 1}),\n",
       " Document(page_content='Clustering is a method of unsupervised learning that involves grouping a set of objects in such a \\nway that objects in the same group (cluster) are more similar to each other than to those in other \\ngroups.  \\n• k-Means Clustering : Partitions the data into k clusters by minimizing the variance \\nwithin each cluster.  \\n• Hierarchical Clustering : Builds a hierarchy of clusters either through a bottom -up \\n(agglomerative) or top -down (divisive) approach.  \\n• DBSCAN (Density -Based Spatial Clustering of Applications with Noise) : Groups \\ntogether points that are close to each other based on a distance measurement, and marks \\npoints that are in low -density regions as outliers.  \\nDimensionality Reduction  \\nDimensionality reduction is the process of reducing the number of random variables under \\nconsideration by obtaining a set of principal variables.  \\n• Principal Component Analysis (PCA) : Projects the data into a lower -dimensional space \\nby maximizing the variance along the principal components.  \\n• t-Distributed Stochastic Neighbor Embedding (t -SNE) : Primarily used for visualizing \\nhigh-dimensional data by reducing it to two or three dimensions.  \\n• Linear Discriminant Analysis (LDA) : Finds the linear combinations of features that \\nbest separate two or more classes of objects or events.  \\nAnomaly Detection  \\nAnomaly detection aims to identify rare items, events, or observations that raise suspicions by \\ndiffering significantly from the majority of the data.  \\n• Statistical Methods : Assume a statistical distribution for the data and identify points that \\ndeviate significantly from this distribution.  \\n• Machine Learning Methods : Include clustering -based methods (e.g., DBSCAN) and \\nmodel -based approaches (e.g., autoencoders).  \\nAssociation Rules  \\nAssociation rule learning is a rule -based machine learning method for discovering interesting \\nrelations between variables in large databases.  \\n• Apriori Algorithm : Identifies frequent itemsets and then derives association rules from \\nthese itemsets.  \\n• Eclat Algorithm : Uses a depth -first search strategy to find frequent itemsets and is often \\nfaster than Apriori for large datasets.  \\n ', metadata={'source': 'file_1.pdf', 'page': 2}),\n",
       " Document(page_content='Supervised Learning  \\nSupervised learning involves training a model on a labeled dataset, which means that each \\ntraining example is paired with an output label. The goal is to learn a mapping from inputs to \\noutputs that can be used to predict the labels of new, unseen examples.  \\nClassification  \\nClassification is a type of supervised learning where the goal is to assign inputs into one of \\nseveral predefined categories.  \\n• Binary Classification : The model distinguishes between two classes. Examples include \\nspam detection (spam or not spam) and disease diagnosis (diseased or healthy).  \\n• Multiclass Classification : The model distinguishes among three or more classes. \\nExamples include handwriting recognition (recognizing each digit from 0 to 9) and image \\nclassification (identifying objects in images).  \\nCommon algorithms for classification include:  \\n• Logistic Regression : Models the probability of a binary outcome using a logistic \\nfunction.  \\n• Decision Trees : Splits the data into subsets based on the value of input features.  \\n• Support Vector Machines (SVM) : Finds the hyperplane that best separates the classes \\nin the feature space.  \\n• k-Nearest Neighbors (k -NN): Classifies a data point based on the majority class among \\nits k-nearest neighbors.  \\nRegression  \\nRegression is a type of supervised learning where the goal is to predict a continuous output.  \\n• Linear Regression : Models the relationship between the input variables and the output \\nby fitting a linear equation to the observed data.  \\n• Polynomial Regression : Extends linear regression by considering polynomial \\nrelationships between the input variables and the output.  \\n• Ridge and Lasso Regression : Variants of linear regression that include regularization \\nterms to prevent overfitting.  \\nCommon metrics for regression include Mean Squared Error (MSE), Root Mean Squared Error \\n(RMSE), and Mean Absolute Error (MAE).  \\nEnsemble Methods  \\nEnsemble methods combine multiple learning algorithms to obtain better predictive performance \\nthan could be obtained from any of the constituent models alone.  ', metadata={'source': 'file_1.pdf', 'page': 0}),\n",
       " Document(page_content='• Random Forests : An ensemble of decision trees, typically trained with the \"bagging\" \\nmethod. Each tree in the forest is trained on a subset of the data, and the final prediction \\nis made by averaging the predictions of all the trees.  \\n• Gradient Boosting : Builds an ensemble of trees sequentially, where each new tree \\nfocuses on correcting the errors made by the previous trees. Examples include Gradient \\nBoosting Machines (GBM) and XGBoost.  \\nSupport Vector Machines (SVM)  \\nSVMs are supervised learning models that analyze data for classification and regression analysis. \\nThe core idea is to find a hyperplane in a high -dimensional space that distinctly classifies the \\ndata points.  \\n• Linear SVM : Uses a linear hyperplane to separate the data.  \\n• Non-linear SVM : Uses kernel functions (e.g., RBF, polynomial) to project data into \\nhigher dimensions where a linear separation is possible.  \\nk-Nearest Neighbors (k -NN) \\nk-NN is a simple, instance -based learning algorithm where the class of a data point is determined \\nby the majority class of its k -nearest neighbors in the feature space. It is computationally \\nexpensive during prediction since it involves calculating the dis tance to all training points.  \\nNeural Networks  \\nNeural networks are a set of algorithms inspired by the structure and function of the human \\nbrain, designed to recognize patterns.  \\n• Feedforward Neural Networks (FNN) : The simplest type of neural network where \\ninformation moves in one direction —from input to output.  \\n• Convolutional Neural Networks (CNNs) : Primarily used for image data, CNNs use \\nconvolutional layers to automatically and adaptively learn spatial hierarchies of features.  \\n• Recurrent Neural Networks (RNNs) : Suitable for sequential data, RNNs have \\nconnections that form directed cycles, allowing them to maintain a memory of previous \\ninputs. Long Short -Term Memory (LSTM) networks are a type of RNN designed to \\nhandle long -term dependencies.  \\nUnsupervised Learning  \\nUnsupervised learning involves training a model on data without labeled responses. The goal is \\nto infer the natural structure present within a set of data points.  \\nClustering  ', metadata={'source': 'file_1.pdf', 'page': 1}),\n",
       " Document(page_content='Clustering is a method of unsupervised learning that involves grouping a set of objects in such a \\nway that objects in the same group (cluster) are more similar to each other than to those in other \\ngroups.  \\n• k-Means Clustering : Partitions the data into k clusters by minimizing the variance \\nwithin each cluster.  \\n• Hierarchical Clustering : Builds a hierarchy of clusters either through a bottom -up \\n(agglomerative) or top -down (divisive) approach.  \\n• DBSCAN (Density -Based Spatial Clustering of Applications with Noise) : Groups \\ntogether points that are close to each other based on a distance measurement, and marks \\npoints that are in low -density regions as outliers.  \\nDimensionality Reduction  \\nDimensionality reduction is the process of reducing the number of random variables under \\nconsideration by obtaining a set of principal variables.  \\n• Principal Component Analysis (PCA) : Projects the data into a lower -dimensional space \\nby maximizing the variance along the principal components.  \\n• t-Distributed Stochastic Neighbor Embedding (t -SNE) : Primarily used for visualizing \\nhigh-dimensional data by reducing it to two or three dimensions.  \\n• Linear Discriminant Analysis (LDA) : Finds the linear combinations of features that \\nbest separate two or more classes of objects or events.  \\nAnomaly Detection  \\nAnomaly detection aims to identify rare items, events, or observations that raise suspicions by \\ndiffering significantly from the majority of the data.  \\n• Statistical Methods : Assume a statistical distribution for the data and identify points that \\ndeviate significantly from this distribution.  \\n• Machine Learning Methods : Include clustering -based methods (e.g., DBSCAN) and \\nmodel -based approaches (e.g., autoencoders).  \\nAssociation Rules  \\nAssociation rule learning is a rule -based machine learning method for discovering interesting \\nrelations between variables in large databases.  \\n• Apriori Algorithm : Identifies frequent itemsets and then derives association rules from \\nthese itemsets.  \\n• Eclat Algorithm : Uses a depth -first search strategy to find frequent itemsets and is often \\nfaster than Apriori for large datasets.  \\n ', metadata={'source': 'file_1.pdf', 'page': 2}),\n",
       " Document(page_content='Clustering  \\nClustering is a type of unsupervised learning that involves grouping a set of objects in such a \\nway that objects in the same group (or cluster) are more similar to each other than to those in \\nother groups.  \\nk-Means Clustering  \\n• Concept : k-Means is one of the simplest and most popular clustering algorithms. It \\npartitions the dataset into k clusters, where each data point belongs to the cluster with the \\nnearest mean.  \\n• Algorithm : \\n1. Initialize k cluster centroids randomly.  \\n2. Assign each data point to the nearest centroid.  \\n3. Recalculate the centroids as the mean of all points assigned to each cluster.  \\n4. Repeat steps 2 and 3 until convergence (i.e., centroids no longer change \\nsignificantly).  \\n• Strengths : Simple and fast for small to medium -sized datasets.  \\n• Weaknesses : Sensitive to initial centroid positions, may converge to a local minimum, \\nnot suitable for non -globular clusters or clusters of different sizes.  \\nHierarchical Clustering  \\n• Concept : Hierarchical clustering creates a hierarchy of clusters using either a top -down \\n(divisive) or bottom -up (agglomerative) approach.  \\n• Algorithm : \\no Agglomerative : \\n1. Start with each data point as its own cluster.  \\n2. Merge the closest pair of clusters.  \\n3. Repeat until all points are in a single cluster.  \\no Divisive : \\n1. Start with all points in one cluster.  \\n2. Recursively split clusters until each point is its own cluster.  \\n• Strengths : No need to specify the number of clusters in advance, produces a dendrogram \\nfor visualization.  \\n• Weaknesses : Computationally intensive, especially for large datasets, sensitive to noise \\nand outliers.  \\nDBSCAN (Density -Based Spatial Clustering of Applications with Noise)  \\n• Concept : DBSCAN groups together points that are closely packed and marks points that \\nare isolated in low -density regions as outliers.  \\n• Algorithm : \\n1. Define parameters ε (epsilon, the maximum radius of the neighborhood) and \\nMinPts  (minimum number of points in the neighborhood).  \\n2. Classify points as core points, reachable points, or outliers.  ', metadata={'source': 'file_2.pdf', 'page': 0}),\n",
       " Document(page_content='3. Expand clusters from core points.  \\n• Strengths : Can find arbitrarily shaped clusters, robust to noise and outliers.  \\n• Weaknesses : Not suitable for datasets with varying densities, sensitive to parameter \\nselection.  \\nDimensionality Reduction  \\nDimensionality reduction involves reducing the number of random variables under \\nconsideration, making the data easier to visualize and often improving algorithm performance.  \\nPrincipal Component Analysis (PCA)  \\n• Concept : PCA transforms the data to a new coordinate system where the greatest \\nvariance lies on the first axis, the second greatest variance on the second axis, and so on.  \\n• Algorithm : \\n1. Standardize the data.  \\n2. Calculate the covariance matrix.  \\n3. Calculate the eigenvalues and eigenvectors of the covariance matrix.  \\n4. Sort eigenvalues and eigenvectors.  \\n5. Select the top k eigenvectors to form a new feature space.  \\n• Strengths : Reduces complexity, improves computational efficiency, captures the most \\nimportant variance in the data.  \\n• Weaknesses : Linear method, may not capture complex structures, sensitive to outliers.  \\nt-Distributed Stochastic Neighbor Embedding (t -SNE)  \\n• Concept : t-SNE is a non -linear dimensionality reduction technique that is particularly \\nwell-suited for visualizing high -dimensional data in 2 or 3 dimensions.  \\n• Algorithm : \\n1. Compute pairwise similarities between points in the high-dimensional space.  \\n2. Define a probability distribution over pairs of points.  \\n3. Define a similar distribution in the lower -dimensional space.  \\n4. Minimize the Kullback -Leibler divergence between the two distributions.  \\n• Strengths : Excellent for visualization, preserves local structure.  \\n• Weaknesses : Computationally intensive, difficult to interpret, sensitive to \\nhyperparameters.  \\nLinear Discriminant Analysis (LDA)  \\n• Concept : LDA is a supervised dimensionality reduction technique that aims to find a \\nlinear combination of features that best separate two or more classes.  \\n• Algorithm : \\n1. Compute the within -class and between -class scatter matrices.  \\n2. Compute the eigenvalues and eigenvectors of the scatter matrices.  \\n3. Select the top eigenvectors to form a new feature space.  \\n• Strengths : Effective for classification tasks, maximizes class separability.  ', metadata={'source': 'file_2.pdf', 'page': 1}),\n",
       " Document(page_content='• Weaknesses : Assumes normal distribution and equal covariance, limited to linear \\nboundaries.  \\nSemi -Supervised Learning  \\nSemi -supervised learning combines a small amount of labeled data with a large amount of \\nunlabeled data during training.  \\nMethods Combining Labeled and Unlabeled Data  \\n• Self-Training : Use a model trained on labeled data to label the unlabeled data, and then \\nretrain the model on the combined data.  \\n• Co-Training : Train two models on different views of the data and use each to label the \\nunlabeled data for the other.  \\n• Graph -Based Methods : Represent data as a graph and use label propagation algorithms \\nto spread labels through the graph.  \\n• Generative Models : Use generative models like variational autoencoders (VAEs) to \\nlearn the underlying distribution of the data and generate pseudo -labels.  \\nReinforcement Learning  \\nReinforcement learning (RL) involves an agent that learns to make decisions by taking actions in \\nan environment to maximize cumulative reward.  \\nQ-Learning  \\n• Concept : Q-Learning is a model -free RL algorithm that learns the value of actions in a \\ngiven state to find an optimal policy.  \\n• Algorithm : \\n1. Initialize Q -values arbitrarily.  \\n2. For each episode:  \\n▪ For each step in the episode:  \\n▪ Choose an action using an exploration -exploitation strategy.  \\n▪ Take the action and observe the reward and next state.  \\n▪ Update the Q -value using the Bellman equation.  \\n• Strengths : Simple, effective for discrete action spaces.  \\n• Weaknesses : Inefficient for large state spaces, slow convergence.  \\nDeep Q -Networks (DQNs)  \\n• Concept : DQNs combine Q -Learning with deep neural networks to handle high -\\ndimensional state spaces.  \\n• Algorithm : \\n1. Use a neural network to approximate the Q -function.  \\n2. Use experience replay to store and sample past experiences.  \\n3. Use target networks to stabilize training.  ', metadata={'source': 'file_2.pdf', 'page': 2}),\n",
       " Document(page_content=\"4. Update the Q -network using the Q -learning update rule.  \\n• Strengths : Handles high -dimensional inputs like images.  \\n• Weaknesses : Requires large amounts of data and computational resources, sensitive to \\nhyperparameters.  \\nPolicy Gradient Methods  \\n• Concept : Policy gradient methods optimize the policy directly by maximizing the \\nexpected reward.  \\n• Algorithm : \\n1. Parameterize the policy with a neural network.  \\n2. Collect trajectories by running the policy in the environment.  \\n3. Compute the gradient of the expected reward with respect to the policy \\nparameters.  \\n4. Update the policy parameters using gradient ascent.  \\n• Strengths : Effective for continuous action spaces, can learn stochastic policies.  \\n• Weaknesses : High variance in gradient estimates, sample inefficient.  \\nMulti -Armed Bandits  \\n• Concept : Multi -armed bandits are a simpler form of RL where an agent repeatedly \\nchooses from a set of actions (arms) to maximize cumulative reward.  \\n• Algorithm : \\n1. Epsilon -Greedy : Choose a random action with probability ε, otherwise choose \\nthe best -known action.  \\n2. UCB (Upper Confidence Bound) : Select actions based on a balance of \\nexploitation and exploration.  \\n3. Thompson Sampling : Use probability matching to sample actions according to \\ntheir likelihood of being optimal.  \\n• Strengths : Simple, effective for online learning scenarios.  \\n• Weaknesses : Limited to static environments, doesn't handle delayed rewards well.  \\nMarkov Decision Processes (MDPs)  \\n• Concept : MDPs provide a mathematical framework for modeling decision -making with \\nstates, actions, rewards, and state transitions.  \\n• Algorithm : \\n1. Define the state space, action space, reward function, and transition probabilities.  \\n2. Solve for the optimal policy using dynamic programming methods like Value \\nIteration or Policy Iteration.  \\n• Strengths : Provides a solid theoretical foundation, can handle stochastic environments.  \\n• Weaknesses : Computationally expensive for large state/action spaces, assumes full \\nknowledge of the environment.  \\n \", metadata={'source': 'file_2.pdf', 'page': 3}),\n",
       " Document(page_content='Deep Learning  \\nDeep learning is a subset of machine learning involving neural networks with many layers. It is \\nused for a variety of tasks such as image and speech recognition, natural language processing, \\nand generative modeling.  \\nConvolutional Neural Networks (CNNs)  \\n• Concept : CNNs are specialized neural networks designed for processing structured grid \\ndata such as images.  \\n• Architecture : \\no Convolutional Layers : Apply filters to input data to create feature maps. These \\nlayers detect local patterns such as edges in images.  \\no Pooling Layers : Downsample the feature maps to reduce dimensionality and \\ncomputational load. Common types include max pooling and average pooling.  \\no Fully Connected Layers : Operate at the end of the network to make final \\npredictions based on the extracted features.  \\n• Strengths : Highly effective for image and video processing tasks, automatic feature \\nextraction, translation invariance.  \\n• Weaknesses : Requires a large amount of labeled data, computationally intensive.  \\nRecurrent Neural Networks (RNNs) and Long Short -Term Memory (LSTM)  \\n• Concept : RNNs are designed for sequential data, where connections between nodes form \\na directed graph along a temporal sequence.  \\n• Architecture : \\no Recurrent Layers : Maintain a hidden state that captures information about \\nprevious inputs in the sequence. Each output is dependent on the previous \\ncomputations.  \\no LSTM Units : Address the vanishing gradient problem of standard RNNs by \\nusing gating mechanisms (input gate, forget gate, output gate) to control the flow \\nof information.  \\n• Strengths : Suitable for time series data, natural language processing, and any task where \\nthe order of inputs matters.  \\n• Weaknesses : Standard RNNs suffer from vanishing/exploding gradient problems, \\nLSTMs are complex and computationally expensive.  \\nGenerative Adversarial Networks (GANs)  \\n• Concept : GANs consist of two neural networks, a generator and a discriminator, that are \\ntrained together through adversarial learning.  \\n• Architecture : \\no Generator : Creates fake data from random noise.  \\no Discriminator : Tries to distinguish between real and fake data.  \\no The generator aims to produce data that is indistinguishable from real data, while \\nthe discriminator aims to improve its accuracy in identifying real vs. fake data.  ', metadata={'source': 'file_3.pdf', 'page': 0}),\n",
       " Document(page_content='• Strengths : Capable of generating high -quality synthetic data, useful for image \\ngeneration, style transfer, and data augmentation.  \\n• Weaknesses : Difficult to train, prone to mode collapse where the generator produces \\nlimited varieties of outputs.  \\nAutoencoders  \\n• Concept : Autoencoders are neural networks used to learn efficient codings of input data, \\ntypically for the purposes of dimensionality reduction or feature learning.  \\n• Architecture : \\no Encoder : Compresses the input data into a latent -space representation.  \\no Decoder : Reconstructs the input data from the latent representation.  \\no Variational Autoencoders (VAEs) : A type of autoencoder that provides a \\nprobabilistic manner for describing an observation in latent space.  \\n• Strengths : Useful for noise reduction, data compression, and feature extraction.  \\n• Weaknesses : Reconstructed data might not be perfect, requires careful tuning of \\narchitecture and hyperparameters.  \\nTransformer Models (e.g., BERT, GPT)  \\n• Concept : Transformers are a type of neural network architecture designed to handle \\nsequential data, replacing traditional RNNs by using self -attention mechanisms.  \\n• Architecture : \\no Self-Attention Mechanism : Allows the model to weigh the importance of \\ndifferent words in a sentence when encoding a particular word.  \\no Encoder -Decoder Structure : Standard transformer architecture includes an \\nencoder to process the input sequence and a decoder to generate the output \\nsequence.  \\no Pre-trained Models (BERT, GPT) : Models like BERT (Bidirectional Encoder \\nRepresentations from Transformers) and GPT (Generative Pre -trained \\nTransformer) are pre -trained on large corpora and fine -tuned for specific tasks.  \\n• Strengths : Superior performance on NLP tasks, parallelizable for efficient training, \\ncaptures long -range dependencies.  \\n• Weaknesses : Requires substantial computational resources, can be difficult to interpret.  \\nTransfer Learning  \\nTransfer learning involves leveraging a pre -trained model on a new but related task, reducing the \\nneed for large amounts of labeled data.  \\nPre-trained Models  \\n• Concept : Models are pre -trained on large datasets and then fine -tuned on a smaller, task -\\nspecific dataset.  \\n• Common Use Cases : Image classification (e.g., using models like VGG, ResNet), \\nnatural language processing (e.g., using models like BERT, GPT).  ', metadata={'source': 'file_3.pdf', 'page': 1}),\n",
       " Document(page_content='• Strengths : Reduces training time and data requirements, often achieves better \\nperformance on small datasets.  \\n• Weaknesses : Fine -tuning might still require considerable resources, pre -trained models \\nmight not generalize well to very different tasks.  \\nDomain Adaptation  \\n• Concept : A form of transfer learning where the model is adapted from one domain \\n(source) to another (target).  \\n• Techniques : \\no Instance -based : Reweighting instances from the source domain to make them \\nmore similar to the target domain.  \\no Feature -based : Learning a common feature space where both source and target \\ndomain data have similar distributions.  \\no Parameter -based : Sharing parameters between source and target domain models \\nor adapting source model parameters to fit the target domain.  \\n• Strengths : Useful when there is a domain shift between the training and target datasets, \\nhelps improve generalization.  \\n• Weaknesses : Complex and computationally expensive, may not always improve \\nperformance if domains are too different.  \\n ', metadata={'source': 'file_3.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ce16cbe-46c5-46c7-9b54-1089b69f9f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5566410a-6f31-4c07-8938-d6d7b090be7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2518f57b-f8f0-4bd9-83a5-761681ddf8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5403025-6330-414d-b319-22aed43d1773",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OllamaEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac30087a-ab0a-4616-aa1f-7ba3992b3066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text1 = \"dogs are good\"\n",
    "# text2 = \"dogs are better\"\n",
    "# text3 = \"israel is a country\"\n",
    "\n",
    "# sentence1 = \"i like dogs\"\n",
    "# sentence2 = \"i like canines\"\n",
    "\n",
    "# emb_1 = embeddings.embed_query(text1)\n",
    "# emb_2 = embeddings.embed_query(text2)\n",
    "# emb_3 = embeddings.embed_query(text3)\n",
    "\n",
    "# from scipy.spatial.distance import cosine\n",
    "# cosine_similarity = 1 - cosine(emb_1, emb_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "572acfdb-eca7-49ce-be07-b95c69270a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.vectorstores import Chroma\n",
    "# from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "persist_directory = 'docs/chroma/file_n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5e765ab-f71a-49bf-a563-1f4ca5b8833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1736717a-0dc9-4802-b4d1-e2a20ee52018",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "08760702-30bf-42a9-b7d2-9d3070cc4123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "414815a1-f9df-4d11-873c-96c40ad6f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd39810e-5e5e-4ff5-b792-56a22c74a298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x289db4dba60>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f89facd4-a9aa-4219-b4b8-371e151a6f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d570f6d-5cde-4927-a153-ca77869f4f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2891e2c-e891-4d12-83bc-cb7397ff1f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
