{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking simple question without providing an history or context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain_together import Together\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Why did the ice cream cone go to the party?\\n\\nBecause it was a scream!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Together(\n",
    "    model=\"META-LLAMA/LLAMA-2-7B-CHAT-HF\",\n",
    "    together_api_key=\"24cdbdf50106e08f6ba3328ac07f97a73eb440ae36da6cdd72f9b091ccca850a\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.together.xyz/v1\",\n",
    "    api_key=\"24cdbdf50106e08f6ba3328ac07f97a73eb440ae36da6cdd72f9b091ccca850a\",\n",
    "    model=\"META-LLAMA/LLAMA-2-7B-CHAT-HF\",)\n",
    "    # model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prompt engineering is a technique used in language learning models (LLMs) to generate high-quality responses to given prompts or inputs. Here are some general steps to perform prompt engineering in LLMs:\n",
      "\n",
      "1. Define the task: Determine the task or problem you want the LLM to perform. This could be generating text, answers to questions, or even completing a task.\n",
      "2. Design the prompt: Create a prompt that accurately reflects the task you want the LLM to perform. This prompt should be clear, concise, and unambiguous.\n",
      "3. Train the LLM: Train the LLM on a large dataset of examples that include the prompt and the desired output. The more diverse and extensive the training data, the better the LLM will be at generating high-quality responses.\n",
      "4. Evaluate the LLM: Once the LLM has been trained, evaluate its performance on a separate test set to determine how well it can generate high-quality responses to the prompts. You can use metrics such as perplexity, accuracy, or ROUGE score to evaluate the LLM's performance.\n",
      "5. Refine the prompt: Based on the results of the evaluation, refine the prompt to improve the LLM's performance. This may involve modifying the language used in the prompt, adding more context, or providing more specific examples.\n",
      "6. Repeat the process: Continue to train, evaluate, and refine the prompt until the LLM is able to generate high-quality responses to the prompts.\n",
      "7. Use the prompt: Once you are satisfied with the performance of the LLM, use the prompt to generate responses to new inputs or prompts.\n",
      "\n",
      "Some specific techniques for prompt engineering in LLMs include:\n",
      "\n",
      "1. Using attention mechanisms: Attention mechanisms allow the LLM to focus on specific parts of the input when generating a response. This can help the LLM to better understand the context and generate more relevant responses.\n",
      "2. Using reinforcement learning: Reinforcement learning can be used to train the LLM to generate responses that are more likely to be correct or relevant. The LLM is trained to receive rewards or penalties based on the quality of its responses, which can help to improve its performance over time.\n",
      "3. Using multi-modal input: Many LLMs can process and generate text, images, and other forms of media. Using multi-modal input can help the LLM to better understand the context and generate more relevant responses.\n",
      "4. Using transfer learning: Transfer learning involves using a pre-trained LLM and fine-tuning it on a specific task or dataset. This can help the LLM to learn the specific patterns and structures of the task more quickly and generate more accurate responses.\n",
      "5. Using data augmentation: Data augmentation involves modifying the training data to increase its diversity and relevance. This can help the LLM to learn more robust patterns and generate more accurate responses.\n",
      "6. Using prompt engineering techniques: There are several techniques specifically designed for prompt engineering, such as textual entailment, question answering, and natural language inference. These techniques can help the LLM to better understand the context and generate more relevant responses.\n",
      "\n",
      "Overall, prompt engineering is a powerful technique for improving the performance of LLMs on a wide range of tasks. By carefully designing and refining the prompts, you can help the LLM to generate high-quality responses and improve its performance over time.\n"
     ]
    }
   ],
   "source": [
    "x = model.invoke(\"how to do prompt engineering in llms\")\n",
    "print(x.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
    "# prompt_value.to_messages()\n",
    "# prompt_value.to_string()\n",
    "prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='  Why did the ice cream cone go to the party?\\n\\nBecause he was feeling cold and wanted to get scooped up!' response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 19, 'total_tokens': 50}, 'model_name': 'META-LLAMA/LLAMA-2-7B-CHAT-HF', 'system_fingerprint': None, 'finish_reason': 'eos', 'logprobs': None} id='run-60409bfe-5639-4847-9492-0ef9fade80b5-0'\n"
     ]
    }
   ],
   "source": [
    "message = model.invoke(prompt_value)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Why did the ice cream cone go to the party?\\n\\nBecause he was feeling cold and wanted to get scooped up!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatting with a model where some context is also provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Based on the provided context, the answer to the question \"where did Harrison work?\" is:\\n\\nKensho'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding = OllamaEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='bears like to eat honey'),\n",
       " Document(page_content='harrison worked at kensho')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Why did the ice cream cone go to the party?\\n\\nBecause it was a scream!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_together import Together\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.together.xyz/v1\",\n",
    "    api_key=\"24cdbdf50106e08f6ba3328ac07f97a73eb440ae36da6cdd72f9b091ccca850a\",\n",
    "    model=\"META-LLAMA/LLAMA-2-7B-CHAT-HF\",)\n",
    "    # model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)\n",
    "\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Why did the ice cream cone go to the party?\n",
      "\n",
      "Because it was feeling a little \"scooped\" up!"
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream(\"ice cream\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  Why did the ice cream cone go to the party?\\n\\nBecause it was a scream!',\n",
       " '  Why did the spaghetti refuse to get dressed?\\n\\nBecause it already had a saucy attitude!',\n",
       " '  Why did the dumpling go to the party?\\n\\nBecause it was a filling good time!']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Why did the ice cream cone go to the party? Because it was feeling sprinkled!'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chain.ainvoke(\"ice cream\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
