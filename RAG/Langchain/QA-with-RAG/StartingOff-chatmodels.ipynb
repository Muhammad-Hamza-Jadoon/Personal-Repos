{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain_together import Together\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Together(\n",
    "    model=\"META-LLAMA/LLAMA-2-7B-CHAT-HF\",\n",
    "    together_api_key=\"24cdbdf50106e08f6ba3328ac07f97a73eb440ae36da6cdd72f9b091ccca850a\"\n",
    ")\n",
    "\n",
    "chat_model = ChatOpenAI(\n",
    "    base_url=\"https://api.together.xyz/v1\",\n",
    "    api_key=\"24cdbdf50106e08f6ba3328ac07f97a73eb440ae36da6cdd72f9b091ccca850a\",\n",
    "    model=\"META-LLAMA/LLAMA-2-7B-CHAT-HF\",)\n",
    "    # model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)\n",
    "    \n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('content', \"  Ah, an excellent question! Model regularization is a technique used in machine learning to prevent overfitting and improve the generalization performance of a model. Overfitting occurs when a model is trained too well on the training data and becomes too specialized, resulting in poor performance on new, unseen data.\\n\\nThe purpose of regularization is to add a penalty term to the loss function that encourages the model to find a simpler solution that generalizes better to new data. This is done by adding a term to the loss function that is proportional to the magnitude of the model's weights or the complexity of the model.\\n\\nThere are several types of regularization, including:\\n\\n1. L1 regularization (Lasso): This adds a penalty term to the loss function that is proportional to the absolute value of the model's weights. This encourages the model to have smaller weights, which in turn helps to prevent overfitting.\\n2. L2 regularization (Ridge): This adds a penalty term to the loss function that is proportional to the square of the model's weights. This also encourages the model to have smaller weights, but it is less sensitive to outliers than L1 regularization.\\n3. Dropout regularization: This is a form of regularization that is applied during training by randomly setting a fraction of the model's neurons to zero. This helps to prevent overfitting by making the model less reliant on any single neuron.\\n4. Early stopping: This is a regularization technique that involves monitoring the validation loss during training and stopping the training process when the validation loss stops improving. This helps to prevent overfitting by stopping the training process before the model has a chance to converge to a poor local minimum.\\n\\nBy using regularization techniques, we can improve the generalization performance of a model and reduce the risk of overfitting, which can lead to better predictive accuracy and a more robust model.\")\n",
      "('additional_kwargs', {})\n",
      "('response_metadata', {'token_usage': {'completion_tokens': 411, 'prompt_tokens': 37, 'total_tokens': 448}, 'model_name': 'META-LLAMA/LLAMA-2-7B-CHAT-HF', 'system_fingerprint': None, 'finish_reason': 'eos', 'logprobs': None})\n",
      "('type', 'ai')\n",
      "('name', None)\n",
      "('id', 'run-2e1c5fe1-35e9-4125-85d3-22ae6821798b-0')\n",
      "('example', False)\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"What is the purpose of model regularization?\"),\n",
    "]\n",
    "\n",
    "x = chat_model.invoke(messages)\n",
    "for i in (x):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
