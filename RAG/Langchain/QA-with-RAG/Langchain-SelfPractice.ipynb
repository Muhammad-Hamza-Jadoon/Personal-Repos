{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain_together import Together\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Together(\n",
    "    model=\"META-LLAMA/LLAMA-2-7B-CHAT-HF\",\n",
    "    together_api_key=\"24cdbdf50106e08f6ba3328ac07f97a73eb440ae36da6cdd72f9b091ccca850a\"\n",
    ")\n",
    "\n",
    "chat_model = ChatOpenAI(\n",
    "    base_url=\"https://api.together.xyz/v1\",\n",
    "    api_key=\"24cdbdf50106e08f6ba3328ac07f97a73eb440ae36da6cdd72f9b091ccca850a\",\n",
    "    model=\"META-LLAMA/LLAMA-2-7B-CHAT-HF\",)\n",
    "    # model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)\n",
    "    \n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM and Chatmodel\n",
    "The main difference between them is their input and output schemas. The LLM objects take string as input and output string. The ChatModel objects take a list of messages as input and output a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes colorful socks?'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Great! \"Programmation\" is the French word for \"programming\".\n",
      "\n",
      "So, \"Je adore le programmation.\" (I love programming.)\n",
      "\n",
      "Would you like me to translate something else for you? \n",
      "\n",
      "--------------------------------------------------------------\n",
      "\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to French.'), HumanMessage(content='I love programming.')] \n",
      "\n",
      "('content', '  Great! I\\'m glad to hear that you enjoy programming. In French, we would say \"Je suis passionnÃ© par le programming.\"\\n\\nIf you want to say \"I love programming\" in French, you can use the phrase \"Je adore le programming.\"\\n\\nDo you have any other questions or sentences you would like me to translate?')\n",
      "('additional_kwargs', {})\n",
      "('response_metadata', {'token_usage': {'completion_tokens': 74, 'prompt_tokens': 38, 'total_tokens': 112}, 'model_name': 'META-LLAMA/LLAMA-2-7B-CHAT-HF', 'system_fingerprint': None, 'finish_reason': 'eos', 'logprobs': None})\n",
      "('type', 'ai')\n",
      "('name', None)\n",
      "('id', 'run-46470bd8-871f-4b2b-8bc0-fa0af18867bf-0')\n",
      "('example', False)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "# With Chain\n",
    "input_dict = {\n",
    "    \"input_language\": \"English\",\n",
    "    \"output_language\": \"French\",\n",
    "    \"text\": \"I love programming.\"\n",
    "}\n",
    "chain = chat_prompt | chat_model | output_parser\n",
    "output = chain.invoke(input_dict)\n",
    "print(output,'\\n\\n--------------------------------------------------------------\\n')\n",
    "\n",
    "# Without Chain\n",
    "message = chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n",
    "print(message,'\\n')\n",
    "response = chat_model.invoke(message)\n",
    "for i in response:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('content', \"  Oh, my dear! ðŸ˜Š Eating tasty things is the best thing since sliced bread! ðŸžðŸ¥– It's like a party in your mouth, and you get to be the guest of honor! ðŸŽ‰ Why, I even heard that eating tasty food can make you feel happier and more energized! ðŸ˜ƒ So, go on, give those tasty treats a try and see for yourself! ðŸ˜„\")\n",
      "('additional_kwargs', {})\n",
      "('response_metadata', {'token_usage': {'completion_tokens': 114, 'prompt_tokens': 54, 'total_tokens': 168}, 'model_name': 'META-LLAMA/LLAMA-2-7B-CHAT-HF', 'system_fingerprint': None, 'finish_reason': 'eos', 'logprobs': None})\n",
      "('type', 'ai')\n",
      "('name', None)\n",
      "('id', 'run-f06ecda3-e7a8-4426-a994-e15f100eaab1-0')\n",
      "('example', False)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that re-writes the user's text to \"\n",
    "                \"sound more upbeat.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# chat_value = chat_prompt.invoke({\"text\": \"i dont like eating tasty things.\"})\n",
    "# for i in (chat_value):\n",
    "#     print(i)\n",
    "# response = chat_model.invoke(chat_value)\n",
    "# for i in response:\n",
    "#     print(i)\n",
    "\n",
    "for i in chat_prompt:\n",
    "    print(i)\n",
    "    \n",
    "# With Chain\n",
    "input_dict = {\n",
    "    \"text\": \"Im very sad right now.\"\n",
    "}\n",
    "chain = chat_prompt | chat_model | output_parser\n",
    "output = chain.invoke(input_dict)\n",
    "print('\\n\\n',output,'\\n','------------------------------------------------------------------------------------------------','\\n')\n",
    "\n",
    "# Without Chain\n",
    "messages = chat_prompt.format_messages(text=\"I don't like eating tasty things\")\n",
    "response = chat_model.invoke(messages)\n",
    "for i in response:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='May the force be with you' role='Jedi' \n",
      "\n",
      "('content', '  Sure! Here is an example of how you could implement a decision tree from scratch in Python:\\n```\\nclass DecisionTree:\\n    def __init__(self, data, target):\\n        \"\"\"\\n        Initialize the decision tree with the given training data and target variable.\\n\\n        :param data: A list of feature arrays.\\n        :param target: The target variable.\\n        \"\"\"\\n        self.data = data\\n        self.target = target\\n        self.tree = None\\n\\n    def fit(self, num_iterations):\\n        \"\"\"\\n        Train the decision tree on the given training data.\\n\\n        :param num_iterations: The number of iterations to train the tree.\\n        \"\"\"\\n        self.tree = self._train_tree(num_iterations)\\n\\n    def _train_tree(self, num_iterations):\\n        \"\"\"\\n        Train the decision tree on the given training data.\\n\\n        :param num_iterations: The number of iterations to train the tree.\\n        :return: The trained decision tree.\\n        \"\"\"\\n        # Initialize the tree\\n        root = Node(self.data[0], self.target[0])\\n        root.left = Node(self.data[1], self.target[1])\\n        root.right = Node(self.data[2], self.target[2])\\n        root.children = [root.left, root.right]\\n        self.tree = root\\n\\n        # Train the tree on the next iteration\\n        for i in range(num_iterations):\\n            # Select the best feature to split\\n            best_feature = None\\n            best_gain = -1\\n            for feature in self.data[:len(self.data) // 2]:\\n                # Calculate the gain of splitting the feature\\n                gain = self._gain(feature, self.target)\\n                if best_feature is None or gain > best_gain:\\n                    best_feature = feature\\n                    best_gain = gain\\n            # Split the data along the best feature\\n            self.split(best_feature, self.data, self.target)\\n            # Recursively train the tree on the left and right subdatasets\\n            self.tree = self._train_tree(num_iterations - 1)\\n            self.tree.left = Node(self.data[len(self.data) // 2:len(self.data) // 2 + len(self.data[len(self.data) // 2])], self.target[len(self.data) // 2:len(self.data) // 2 + len(self.data[len(self.data) // 2])])\\n            self.tree.right = Node(self.data[len(self.data) // 2 + len(self.data[len(self.data) // 2]):], self.target[len(self.data) // 2 + len(self.data[len(self.data) // 2]):])\\n            self.tree.children = [self.tree.left, self.tree.right]\\n\\n    def _gain(self, feature, target):\\n        \"\"\"\\n        Calculate the gain of splitting the given feature.\\n\\n        :param feature: The feature to split.\\n        :param target: The target variable.\\n        :return: The gain of splitting the feature.\\n        \"\"\"\\n        # Calculate the number of instances in each subset\\n        num_instances_left = len(self.data[:len(self.data) // 2])\\n        num_instances_right = len(self.data[len(self.data) // 2:])\\n        # Calculate the gain of splitting the feature\\n        gain = num_instances_left * self.target[len(self.data) // 2:len(self.data) // 2 + len(self.data[len(self.data) // 2])] +\\n               num_instances_right * (1 - self.target[len(self.data) // 2:len(self.data) // 2 + len(self.data[len(self.data) // 2])])\\n        return gain\\n\\n    def split(self, feature, data, target):\\n        \"\"\"\\n        Split the data along the given feature and update the tree.\\n\\n        :param feature: The feature to split.\\n        :param data: The data to split.\\n        :param target: The target variable.\\n        \"\"\"\\n        # Find the splitting point\\n        split_index = self.find_split_index(feature, data, target)\\n        # Split the data along the feature\\n        self.data[:len(self.data) // 2] = data[:split_index]\\n        self.data[len(self.data) // 2:] = data[split_index:]\\n        self.target[:len(self.data) // 2] = target[:split_index]\\n        self.target[len(self.data) // 2:] = target[split_index:]\\n        # Update the tree\\n        self.tree.split(feature, split_index)\\n\\n    def find_split_index(self, feature, data, target):\\n        \"\"\"\\n        Find the splitting index for the given feature and data.\\n\\n        :param feature: The feature to split.\\n        :param data: The data to split.\\n        :param target: The target variable.\\n        :return: The splitting index.\\n        \"\"\"\\n        # Calculate the mean and standard deviation of the target variable\\n        mean = np.mean(target)\\n        std = np.std(target)\\n        # Find the splitting index that maximizes the information gain\\n        for i in range(len(data)):\\n            if data[i][feature] < mean:\\n                continue\\n            if data[i][feature] > mean + std * 2:\\n                break\\n        return i\\n\\n    def _train_tree_node(self, node, data, target):\\n        \"\"\"\\n        Train a decision tree node with the given data and target.\\n\\n        :param node: The current node in the tree.\\n        :param data: The data to train the node with.\\n        :param target: The target variable.\\n        :return: The trained node.\\n        \"\"\"\\n        # Calculate the mean and standard deviation of the target variable\\n        mean = np.mean(target)\\n        std = np.std(target)\\n        # Train the node\\n        if len(data) == 1:\\n            # Use the single instance learning algorithm\\n            node.split(data[0][feature], data[0], target)\\n        else:\\n            # Use the majority voting algorithm\\n            node.split(data[:len(data) // 2][feature], data[:len(data) // 2], target)\\n        # Recursively train the left and right child nodes\\n        node.left = self._train_tree_node(Node(data[:len(data) // 2][::-1], target[:len(data) // 2]), data[:len(data) // 2], target[:len(data) // 2])\\n        node.right = self._train_tree_node(Node(data[len(data) // 2:][::-1], target[len(data) // 2:]), data[len(data) // 2:], target[len(data) // 2:])\\n        return node\\n\\n    def _train_tree(self, data):\\n        \"\"\"\\n        Train a decision tree on the given data.\\n\\n        :param data: The data to train the tree with.\\n        :return: The trained decision tree.\\n        \"\"\"\\n        # Initialize the tree\\n        root = Node(data[0], data[0][0])\\n        root.left = Node(data[1], data[1][0])\\n        root.right = Node(data[2], data[2][0])\\n        root.children = [root.left, root.right]\\n        # Train the tree\\n        self.tree = root\\n        self.fit(10)\\n        return self.tree\\n```\\nThis implementation includes the following functions:\\n\\n* `DecisionTree`: The decision tree class.\\n* `fit`: Train the decision tree on the given training data.\\n* `split`: Split the data along the given feature and update the tree.\\n* `find_split_index`: Find the splitting index for the given feature and data.\\n* `train_tree_node`: Train a decision tree node with the given data and target.\\n* `train_tree`: Train a decision tree on the given data.\\n\\nYou can use this implementation by creating an instance of the `DecisionTree` class, passing in the training data and target variable, and then calling the `fit` method to train the tree. You can also use the `split` method to split the data along the best feature and update the tree, and the `find_split_index` method to find the splitting index for the given feature and data.\\n\\nHere is an example of how you could use this implementation:\\n```\\n# Create a decision tree class\\ntree = DecisionTree(data, target)\\n# Train the tree\\ntree.fit(10)\\n# Split the data along the best feature\\ntree.split(data[:len(data) // 2][::-1], data[:len(data) // 2], target[:len(data) // 2])\\n# Print the trained tree\\nprint(tree)\\n```\\nThis will train a decision tree on the given training data and print the trained tree.\\n\\nI hope this helps! Let me know if you have any questions.')\n",
      "('additional_kwargs', {})\n",
      "('response_metadata', {'token_usage': {'completion_tokens': 2184, 'prompt_tokens': 16, 'total_tokens': 2200}, 'model_name': 'META-LLAMA/LLAMA-2-7B-CHAT-HF', 'system_fingerprint': None, 'finish_reason': 'eos', 'logprobs': None})\n",
      "('type', 'ai')\n",
      "('name', None)\n",
      "('id', 'run-1d7a6194-d80c-452f-a43a-fb9f16267105-0')\n",
      "('example', False)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "prompt = \"May the {subject} be with you\"\n",
    "\n",
    "chat_message_prompt = ChatMessagePromptTemplate.from_template(\n",
    "    role=\"Jedi\", template=prompt\n",
    ")\n",
    "message = chat_message_prompt.format(subject=\"force\")\n",
    "print(message,'\\n')\n",
    "\n",
    "# response = chat_model.invoke(\"\")\n",
    "# for i in response:\n",
    "#     print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ask the model to summarize provided conversation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('name', None)\n",
      "('input_variables', ['conversation', 'word_count'])\n",
      "('input_types', {'conversation': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]})\n",
      "('output_parser', None)\n",
      "('partial_variables', {})\n",
      "('metadata', None)\n",
      "('tags', None)\n",
      "('messages', [MessagesPlaceholder(variable_name='conversation'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['word_count'], template='Summarize our conversation so far in {word_count} words.'))])\n",
      "('validate_template', False)\n",
      "\n",
      " [HumanMessage(content='What is the best way to learn programming?'), AIMessage(content='1. Choose a programming language: Decide on a programming language that you want to learn.\\n\\n2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\\n\\n3. Practice, practice, practice: The best way to learn programming is through hands-on experience'), HumanMessage(content='Summarize our conversation so far in 10 words.')]\n",
      "content='What is the best way to learn programming?'\n",
      "content='1. Choose a programming language: Decide on a programming language that you want to learn.\\n\\n2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\\n\\n3. Practice, practice, practice: The best way to learn programming is through hands-on experience'\n",
      "content='Summarize our conversation so far in 10 words.'\n",
      "\n",
      "\n",
      "Ouput:    Learn programming by choosing a language, practicing basics, and coding.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    " \n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [   \n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"conversation\"\n",
    "        ), \n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"Summarize our conversation so far in {word_count} words.\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "for i in chat_prompt:\n",
    "    print(i)\n",
    "    \n",
    "human_message = HumanMessage(content=\"What is the best way to learn programming?\")\n",
    "ai_message = AIMessage(\n",
    "    content=\"\"\"\\\n",
    "1. Choose a programming language: Decide on a programming language that you want to learn.\n",
    "\n",
    "2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\n",
    "\n",
    "3. Practice, practice, practice: The best way to learn programming is through hands-on experience\\\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "format = chat_prompt.format_prompt(\n",
    "    conversation=[human_message, ai_message], word_count=\"10\"\n",
    ").to_messages()\n",
    "print('\\n',format)\n",
    "for i in format:\n",
    "    print(i)\n",
    "\n",
    "chain = chat_prompt | chat_model | output_parser\n",
    "input_dict = {\n",
    "    \"conversation\": \n",
    "    [\n",
    "        human_message,\n",
    "        ai_message,\n",
    "    ],\n",
    "    \"word_count\": \"10\",\n",
    "}\n",
    "output = chain.invoke(input_dict)\n",
    "print(\"\\n\\nOuput: \",output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
